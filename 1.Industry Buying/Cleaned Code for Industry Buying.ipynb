{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b1549e",
   "metadata": {},
   "source": [
    "# Execute all the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02284325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import sys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57792d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function finds all the category which can be used for collecting the datas\n",
    "def get_the_category_urls(url_having_all_categories):\n",
    "    #validate whether the url is present or not\n",
    "    if not validators.url(url_having_all_categories):\n",
    "        return None, None, 1\n",
    "    url = url_having_all_categories\n",
    "    max_retries = 10\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status() # Raises HTTPError for bad responses\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#             print(\"Title:\", soup.title.text)\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "#             print(f\"Attempt {attempt}: Request failed - {e}\")\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "#         print(f\"Failed to retrieve the content after {max_retries} attempts.\")\n",
    "        return None, None, 1\n",
    "\n",
    "    \n",
    "    all_hrefs = [a.get('href') for a in soup.find_all('a', class_='catHead')]\n",
    "    all_hrefs=  [url[:-12]+i for i in all_hrefs]\n",
    "    \n",
    "    category_elements = soup.find_all('a', class_='catHead')\n",
    "    category_names= [element.text for element in category_elements]\n",
    "    \n",
    "    return all_hrefs,category_names, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1764671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what this function do is go inside all the category url and finds all the product's url\n",
    "#we return three thing\n",
    "#1.list_subcategories->having the urls for the product\n",
    "#2.error_logs- lets say if accessing the given_url is done, we can find all the pages and subcategories which failed to load\n",
    "#3.error-if the website fails to load, we can simply return the message 1\n",
    "def find_all_urls(given_url):\n",
    "    error_logs=[]\n",
    "    list_subcategories=[]\n",
    "    url_k=given_url\n",
    "\n",
    "    subcategory_names=[]\n",
    "\n",
    "    # url = \"https://www.hippostores.com/ihbcategory/sanitary-ware-and-bath-fittings-sbf\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    \n",
    "    #validate whether the url exist or not\n",
    "    if not validators.url(url_k):\n",
    "        return list_subcategories, subcategory_names, error_logs, 1\n",
    "    \n",
    "    #prevent from not getting a responce and getting a bad responce\n",
    "    maxtries=5\n",
    "    error=1\n",
    "    for tries in range(maxtries):\n",
    "        \n",
    "        session = requests.Session()\n",
    "        response = session.get(url_k, headers=headers)\n",
    "        if response.status_code == 200 and response.status_code != 400:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            error=0\n",
    "            break\n",
    "\n",
    "    if error==1:\n",
    "        return list_subcategories, subcategory_names, error_logs, 1\n",
    "    \n",
    "    #first we have to find the name of the category\n",
    "    \n",
    "    target_elements = soup.find_all('p', {'class': 'productTitle'})\n",
    "\n",
    "    for element in target_elements:\n",
    "        product_title = element.find('a').contents[0].strip()\n",
    "        subcategory_names.append(product_title)\n",
    "    \n",
    "\n",
    "    #list_category contains all the links for the subcategories in the specific category\n",
    "    list_category=[]\n",
    "    cat_colm_elements = soup.find_all(class_=\"cat-colm\")\n",
    "\n",
    "\n",
    "    # Extract and print the first link from each element\n",
    "    for cat_colm in cat_colm_elements:\n",
    "        href_link = \"https://www.industrybuying.com\"+cat_colm.find('a')['href']\n",
    "        list_category.append(href_link)\n",
    "\n",
    "    #so we get our list category which contains all the links for the subcategories in the specific category\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(len(list_category)):\n",
    "        error_logs.append([])\n",
    "        #this contains all the urls of that specific subcategory\n",
    "        list_of_url_individual=[]\n",
    "\n",
    "        #link_one_category is specific url of a subcategory\n",
    "        link_one_category=list_category[i]\n",
    "        error_logs[-1].append(link_one_category)\n",
    "        headers = {\n",
    "          'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        \n",
    "        maxtries=5\n",
    "        error=1\n",
    "        for tries in range(maxtries):\n",
    "            session = requests.Session()\n",
    "            response = session.get(link_one_category, headers=headers)\n",
    "\n",
    "            #prevent from not getting a responce and getting a bad responce\n",
    "            if response.status_code == 200 and response.status_code != 400:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                error=0\n",
    "                error_logs[-1].append(0) #means this category is successfully captured\n",
    "                error_logs[-1].append([])\n",
    "                break\n",
    "        if error==1:\n",
    "#             error_logs[-1].append(1,[], 0, 0)\n",
    "            error_logs[-1].append(1)\n",
    "            error_logs[-1].append([])\n",
    "            error_logs[-1].append(0)\n",
    "            error_logs[-1].append(0)\n",
    "            continue\n",
    "\n",
    "        span_element = soup.find('span', class_='productslimit')\n",
    "\n",
    "        span_text = span_element.get_text()\n",
    "\n",
    "        span_text_parts = span_text.strip('()').split()\n",
    "\n",
    "        products_one_page = int(span_text_parts[0].split('-')[1])\n",
    "        total_products = int(span_text_parts[-1])\n",
    "        pages=math.ceil(total_products / products_one_page)\n",
    "\n",
    "        #now all we have to do is go page to page and pick all the url having the links\n",
    "        for page in range(pages):\n",
    "            url_page_wise=link_one_category+ f\"?page={page+1}\"\n",
    "\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            maxtries=5\n",
    "            error=1\n",
    "            for tries in range(maxtries):\n",
    "                session = requests.Session()\n",
    "                response = session.get(url_page_wise, headers=headers)\n",
    "                #prevent from not getting a responce and getting a bad responce\n",
    "                if response.status_code == 200 and response.status_code != 400:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    error=0\n",
    "                    break\n",
    "                if error==1:\n",
    "                    error_logs[-1][-1].append(page)\n",
    "                    continue\n",
    "            \n",
    "                \n",
    "\n",
    "            div_elements = soup.find_all('div', class_='AH_ProductView')\n",
    "\n",
    "\n",
    "\n",
    "            # Loop through each div element\n",
    "            for div_element in div_elements:\n",
    "                # Find the first anchor element within the div\n",
    "                first_link = div_element.find('a')\n",
    "\n",
    "                # If a link is found, append its href to the list\n",
    "                if first_link:\n",
    "                    href_link = first_link.get('href')\n",
    "                    list_of_url_individual.append(href_link)\n",
    "        \n",
    "        error_logs[-1].append(len(list_of_url_individual))\n",
    "        list_subcategories.append(list_of_url_individual)\n",
    "        error_logs[-1].append(len(list(set(list_of_url_individual))))\n",
    "#         print(i, end=\" \")\n",
    "    \n",
    "    #remove duplicate from the list_subcategories\n",
    "    list_subcategories_copy=[]\n",
    "    for lis in list_subcategories:\n",
    "        lis_set=list(set(lis))\n",
    "        list_subcategories_copy.append(lis_set)\n",
    "    \n",
    "    list_subcategories=list_subcategories_copy\n",
    "    #we can return this table without any duplicate index\n",
    "    \n",
    "    \n",
    "    return list_subcategories, subcategory_names, error_logs, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc573055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logs_table(logs, category_name, subcategory_names, category_url):\n",
    "    for log in logs:\n",
    "        log[2] = [str(num) for num in log[2]]\n",
    "        lis=log[2]\n",
    "        log[2]=', '.join(lis)\n",
    "        \n",
    "    columns=['Category URL', 'Sub Category URL', 'Category', 'Sub Category', 'Category Check', 'Sub Category Check', 'Pages Check', 'Page Not Retrieved', 'Total Product', 'Non duplicated Products']\n",
    "        \n",
    "    \n",
    "    df=pd.DataFrame(logs, columns=['Sub Category URL','Sub Category Check','Page Not Retrieved','Total Product','Non duplicated Products'])\n",
    "    df['Category URL']=category_url\n",
    "    df['Category']=category_name\n",
    "    df['Sub Category']=subcategory_names\n",
    "    df['Category Check']=0\n",
    "    df['Pages Check'] = df['Page Not Retrieved'].apply(lambda x: 0 if x == '' else 1)\n",
    "    \n",
    "    df=df[columns]\n",
    "    \n",
    "    filename=\"IndustryBuying\"+\"__\"+category_name+\"__\"+\"logs.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea88ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what it does is find all the prices inside the table listing the mrp and other stuff in each product\n",
    "def find_prices(soup):\n",
    "    mrp=None\n",
    "    discount=None\n",
    "    discount_flag=None\n",
    "    \n",
    "    price_text = soup.find('span', class_='mainPrice')\n",
    "    \n",
    "    \n",
    "    if price_text:\n",
    "        price_text=price_text.text\n",
    "        price_integer = int(price_text.replace('Rs.', '').replace(',', ''))\n",
    "        mrp=price_integer\n",
    "        discount_flag=0\n",
    "        discount=\"1-\"+str(mrp)\n",
    "        return mrp, discount, discount_flag, 0\n",
    "    \n",
    "    mrp_listing = soup.find('del', {'id': 'AH_ListPrice'})\n",
    "    bulk_table= soup.find('div', {'class': 'ah-bulk-qty-table'})\n",
    "    per_piece_price=soup.find('span', class_='AH_PricePerPiece')\n",
    "    \n",
    "    if mrp_listing is None and bulk_table is None and per_piece_price is None:\n",
    "        return None, None, None, 1\n",
    "\n",
    "    \n",
    "    \n",
    "    #find a condition with no discount on individual as well as on packs\n",
    "    if mrp_listing is None and bulk_table is None:\n",
    "        discount_flag=0\n",
    "        #now since there wont be any discount either ways we can have our discount as:- mrp-1\n",
    "        mrp=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(mrp)\n",
    "        \n",
    "    \n",
    "    #find a condition with discount only on 1 item \n",
    "    elif mrp_listing and bulk_table is None:\n",
    "        discount_flag=1\n",
    "        #now my discount price and my mrp would be different\n",
    "        discount=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(discount)\n",
    "        mrp=mrp_listing.get_text(strip=True).replace(\",\", \"\")\n",
    "        \n",
    "        \n",
    "    #find a condition where discount is only present on bulk\n",
    "    elif mrp_listing is None and bulk_table:\n",
    "        discount_flag=2\n",
    "        #now since there wont be any discount(individual) we can have our discount initially as:- mrp-1\n",
    "        mrp=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(mrp)\n",
    "        \n",
    "        element_texts = []\n",
    "\n",
    "        # Start the index at 0\n",
    "        index = 0\n",
    "\n",
    "        while True:\n",
    "            # Generate the class name with the current index\n",
    "            class_name = f'table-row bulk-option AH_BulkPriceOption AH_BulkPriceOption_{index}'\n",
    "\n",
    "            # Find all div elements with the specific class\n",
    "            elements = soup.find_all('div', class_=class_name)\n",
    "\n",
    "            # If elements are found, proceed\n",
    "            if elements:\n",
    "                for element in elements:\n",
    "                    # Find all div elements with class \"table-cell\" inside the current element\n",
    "                    table_cells = element.find_all('div', class_='table-cell')\n",
    "\n",
    "                    # Extract text from each \"table-cell\" and add it to the list\n",
    "                    cell_texts = [cell.get_text(strip=True) for cell in table_cells]\n",
    "                    element_texts.append(cell_texts)\n",
    "\n",
    "                # Increment the index for the next iteration\n",
    "                index += 1\n",
    "            else:\n",
    "                # If no elements are found, break the loop\n",
    "                break\n",
    "\n",
    "        # Print the list of texts for each element\n",
    "        for each_element in element_texts:\n",
    "            a=each_element[1]\n",
    "            b=each_element[2]\n",
    "            a=a.split('-')[0].strip().split('+')[0]\n",
    "            b=b.split(\" \")[-1]\n",
    "            #now we got these dudes, we can simple append this to string discount\n",
    "            discount+=\"; \"+a+\"-\"+b\n",
    "        \n",
    "    \n",
    "    #find a condition where the discount is present on both, individual and bulk\n",
    "    else:\n",
    "        discount_flag=3\n",
    "        #now we can different discount for individual and bulk\n",
    "        discount=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(discount)\n",
    "        mrp=mrp_listing.get_text(strip=True).replace(\",\", \"\") \n",
    "        \n",
    "        element_texts = []\n",
    "\n",
    "        # Start the index at 0\n",
    "        index = 0\n",
    "\n",
    "        while True:\n",
    "            # Generate the class name with the current index\n",
    "            class_name = f'table-row bulk-option AH_BulkPriceOption AH_BulkPriceOption_{index}'\n",
    "\n",
    "            # Find all div elements with the specific class\n",
    "            elements = soup.find_all('div', class_=class_name)\n",
    "\n",
    "            # If elements are found, proceed\n",
    "            if elements:\n",
    "                for element in elements:\n",
    "                    # Find all div elements with class \"table-cell\" inside the current element\n",
    "                    table_cells = element.find_all('div', class_='table-cell')\n",
    "\n",
    "                    # Extract text from each \"table-cell\" and add it to the list\n",
    "                    cell_texts = [cell.get_text(strip=True) for cell in table_cells]\n",
    "                    element_texts.append(cell_texts)\n",
    "\n",
    "                # Increment the index for the next iteration\n",
    "                index += 1\n",
    "            else:\n",
    "                # If no elements are found, break the loop\n",
    "                break\n",
    "\n",
    "        # Print the list of texts for each element\n",
    "        for each_element in element_texts:\n",
    "            a=each_element[1]\n",
    "            b=each_element[2]\n",
    "            a=a.split('-')[0].strip().split('+')[0]\n",
    "            b=b.split(\" \")[-1]\n",
    "            #now we got these dudes, we can simple append this to string discount\n",
    "            discount+=\"; \"+a+\"-\"+b\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #our discount will be in integer but our mrp must be integer\n",
    "    mrp=int(mrp)\n",
    "    \n",
    "    \n",
    "    return mrp, discount, discount_flag, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b4d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is an important match\n",
      "Today is an important match\n",
      "Today is an important match\n",
      "Health Health Philips bulbs\n"
     ]
    }
   ],
   "source": [
    "#these function can be used for cleaning\n",
    "#for eg:-name of the product is:-Super Bulb with ultra shining(332743), then my name_cleaner outputs:-Super Bulb with ultra shining\n",
    "def name_cleaner(str):\n",
    "  l=str.split(' ')\n",
    "  clean_str=\"\"\n",
    "  if l[-1].isnumeric():\n",
    "    clean_str=' '.join(l[0:-2])\n",
    "  elif l[-1][1:-1].isnumeric():\n",
    "    clean_str=' '.join(l[0:-1])\n",
    "  else:\n",
    "    clean_str=str\n",
    "\n",
    "  return clean_str\n",
    "\n",
    "print(name_cleaner(\"Today is an important match - 201920\"))\n",
    "print(name_cleaner(\"Today is an important match (38929)\"))\n",
    "print(name_cleaner(\"Today is an important match\"))\n",
    "\n",
    "#string star remover and capitalizer\n",
    "def process_string(input_string):\n",
    "    cleaned_string = input_string.rstrip('.*').lower().capitalize()\n",
    "    return cleaned_string\n",
    "\n",
    "print(process_string(\"health.\"), process_string(\"health*\"), process_string(\"Philips bulbs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4498821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we send our both list through which we can make the dataset\n",
    "def get_dataset(list_mandatory, list_optional, set_with_columns, category_name):\n",
    "    \n",
    "    columns = [\"Primary Key\", \"Brand\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\", \"Product\",\n",
    "               \"Name\", \"Piece Quantity\", \"MRP\", \"Discount Price(Max)\", \"Discount Price(Min)\",\n",
    "               \"Discount Check Flag\", \"URL\"]\n",
    "\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    \n",
    "    df_mandatory = pd.DataFrame(list_mandatory, columns=columns)   \n",
    "    filename=\"IndustryBuying\"+\"__\"+category_name+\"__\"+\"MANDATORY.csv\"\n",
    "    df_mandatory.to_csv(filename,index=False)\n",
    "    #we have made the mandatory dataset\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    # Iterate through each row in list_optional\n",
    "    for row in list_optional:\n",
    "        # Create a dictionary to store values for the current row\n",
    "        row_values = {column: None for column in set_with_columns}\n",
    "\n",
    "        # Update the values for the columns present in the current row\n",
    "        for column, value in row:\n",
    "            row_values[column] = value\n",
    "\n",
    "        # Append the row to the list\n",
    "        data.append(row_values)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_optional = pd.DataFrame(data)\n",
    "    filename=\"IndustryBuying\"+\"__\"+category_name+\"__\"+\"OPTIONAL.csv\"\n",
    "    df_optional.to_csv(filename, index=False)\n",
    "    #we have made the optional dataset\n",
    "    df=df_optional.copy()\n",
    "    # Sort columns based on the number of NaN values in each column\n",
    "    sorted_columns = df_optional.isna().sum().sort_values().index\n",
    "    df = df[sorted_columns]\n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39643dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(list_subcategories, category_name):\n",
    "    list_mandatory=[]\n",
    "    product_with_inconsistency=[]\n",
    "    set_with_columns=set()\n",
    "    list_optional=[]\n",
    "    for i in range(len(list_subcategories)):\n",
    "        for j in range(1):\n",
    "    #         url=\"https://www.industrybuying.com/knapsack-sprayer-agripro-AGR.KNA.45809501/\"\n",
    "\n",
    "            url=\"https://www.industrybuying.com\"+list_subcategories[i][j]\n",
    "\n",
    "\n",
    "            options = Options()\n",
    "            options.add_argument('--headless')\n",
    "\n",
    "            try:\n",
    "                # Attempt to open the Chrome WebDriver\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "\n",
    "                # Attempt to navigate to the given URL\n",
    "                driver.get(url)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle the exception (e.g., print an error message)\n",
    "                product_with_inconsistency.append([url, \"URL inconsistency\"])\n",
    "                # Optionally, you may want to quit the driver if an exception occurs\n",
    "                driver.quit()\n",
    "#                 print(f\"**->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                button = WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main\"]/div/div/div/div/div/div[1]/div[4]/div[2]/div[7]/div[1]/table/tbody/tr[8]/td/a')))\n",
    "                if button:\n",
    "                    button.click()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                # Get the latest HTML content\n",
    "                html_content = driver.page_source\n",
    "            except Exception as e:\n",
    "                product_with_inconsistency.append([url, \"URL inconsistency\"])\n",
    "                driver.quit()\n",
    "#                 print(f\"**->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Use BeautifulSoup to parse the HTML content\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "            #first we have to get the mandotory fields\n",
    "            #it contains:- Primary Key, Brand, Category, Sub Category, Hidden Categories, Product, Name\n",
    "            #MRP, Discount Price, Discount Check Flag, URL\n",
    "\n",
    "\n",
    "            #lets figure out the prices and the discounts and discount flag\n",
    "            mrp, discount, discount_flag, error=find_prices(soup)\n",
    "\n",
    "            if error==1:\n",
    "                product_with_inconsistency.append([url, \"Bad Gateway inconsistency\"])\n",
    "#                 print(f\"Bad Gateway ->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "            discount_max=discount.split(\";\")[0].split(\"-\")[1]\n",
    "            discount_min=discount.split(\"; \")[-1].split(\"-\")[-1]\n",
    "\n",
    "\n",
    "            #lets get the brand\n",
    "            h2_element = soup.find('h2', class_='by')\n",
    "\n",
    "            if h2_element:\n",
    "                a_tag = h2_element.find('a')\n",
    "\n",
    "                if a_tag:\n",
    "                    brand=a_tag.text.strip()\n",
    "\n",
    "\n",
    "            #now we get the category, subcategory, name and the product\n",
    "            links = soup.select('div.commonBreadCrums a')\n",
    "\n",
    "            # Extract the text inside each link and store it in a list\n",
    "            links_list = [link.text.strip() for link in links]\n",
    "\n",
    "            #if we found out the length of the links_list is less than 4, we will continue\n",
    "            if len(links_list)<3:\n",
    "                product_with_inconsistency.append([url, \"Category listing inconsistency\"])\n",
    "#                 print(f\"**->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #we have got all the subcategory using this\n",
    "            l=links_list.copy()\n",
    "            dict_cat={}\n",
    "            for z in range(6):\n",
    "                if len(l)-2>z and z!=5:\n",
    "                    dict_cat[f\"l{z+1}\"]=l[z+1]\n",
    "                else:\n",
    "                    dict_cat[f\"l{z+1}\"]=\"\"\n",
    "\n",
    "                if z==5 and len(l)-2>z:\n",
    "                    dict_cat[f\"l{z+1}\"]=\" \".join([v for v in l[z+1:-1]])       \n",
    "    \n",
    "            product=l[-1]\n",
    "\n",
    "            #now we will have some constants\n",
    "            piece_quantity=1\n",
    "\n",
    "\n",
    "            #now we have to get the name\n",
    "            name = soup.find('span', class_='productTitle').find('h1').text.strip()\n",
    "\n",
    "\n",
    "            #first of all we would be scrapping is optional stuff\n",
    "            dict_table={}\n",
    "\n",
    "            table_element = soup.find('div', {'class': 'tabDetailsContainer', 'id': 'famSpec'})\n",
    "\n",
    "\n",
    "            # Find all div elements with class 'filterRow'\n",
    "            if table_element:\n",
    "                filter_rows = table_element.find_all('div', class_='filterRow')\n",
    "            else:\n",
    "                filter_rows=None\n",
    "            # Initialize an empty list to store the tuple pairs\n",
    "            dict_table={}\n",
    "\n",
    "\n",
    "\n",
    "            # Loop through each filter row and extract feature name and value\n",
    "            if filter_rows:\n",
    "                for row in filter_rows:\n",
    "                    key = row.find('div', class_='featureNamePr').text.strip()\n",
    "                    key=process_string(key)\n",
    "                    value = row.find('div', class_='featureValuePr').text.replace(':', '').strip()\n",
    "                    #now we have got the feature_name and feature_value\n",
    "                    #we can put this in a dictionary\n",
    "                    dict_table[key]=str(value)\n",
    "\n",
    "\n",
    "            #lets consider a scenario, if the model no is not present, we wont be definitely moving forward \n",
    "            u_id=None\n",
    "            if 'Model no' in dict_table:\n",
    "                u_id=dict_table['Model no']\n",
    "            else:\n",
    "                #if model_no not present, then put the id format as:-ibyymmddhhmmssuuuuuu (u denotes microseconds)\n",
    "                current_time = datetime.now()\n",
    "                # Format the time to \"yymmddhhmmssuuuuuu\"\n",
    "                formatted_time = current_time.strftime(\"%y%m%d%H%M%S%f\")[:]  \n",
    "                u_id=\"ib\"+formatted_time\n",
    "\n",
    "            if u_id =='-':\n",
    "                #if model_no not present, then put the id format as:-ibyymmddhhmmssuuuuuu (u denotes microseconds)\n",
    "                current_time = datetime.now()\n",
    "                # Format the time to \"yymmddhhmmssuuuuuu\"\n",
    "                formatted_time = current_time.strftime(\"%y%m%d%H%M%S%f\")[:]  \n",
    "                u_id=\"ib\"+formatted_time\n",
    "                \n",
    "            dict_table['Model no']=u_id \n",
    "\n",
    "            #now we can add all the optional columns in the set\n",
    "            for k in dict_table:\n",
    "                set_with_columns.add(k)\n",
    "\n",
    "\n",
    "            key_value_pairs_as_lists = [[key, value] for key, value in dict_table.items()]\n",
    "            list_optional.append(key_value_pairs_as_lists)\n",
    "            print(i*2+j+1)\n",
    "\n",
    "    #         print(brand)\n",
    "\n",
    "    #         print(category, subcategory, product, name, sep='\\t')\n",
    "\n",
    "    #         print(u_id, discount_flag, mrp, discount_max, discount_min, sep='\\t')\n",
    "\n",
    "            #Primary Key, Brand, Category, Sub Category, Hidden Categories, Product, Name, Piece Quantity, MRP, Discount Price(Max), Discount Price(Min), Discount Check Flag, URL\n",
    "            list_mandatory.append([u_id, brand, dict_cat[\"l1\"], dict_cat[\"l2\"], dict_cat[\"l3\"], dict_cat[\"l4\"], dict_cat[\"l5\"], dict_cat[\"l6\"], product, name, piece_quantity, mrp, discount_max, discount_min, discount_flag, url])\n",
    "#             print(len(list_mandatory))\n",
    "            \n",
    "    #now since we got two things, first is the mandatory list and second is the optional list,\n",
    "    #we can get the dataset from both of them\n",
    "    get_dataset(list_mandatory, list_optional, set_with_columns, category_name)\n",
    "    #what this function does is\n",
    "    \n",
    "\n",
    "    return product_with_inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5401bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The structure of the urls looks like:- (urls-https://www.industrybuying.com/abrasives-642/)\n",
      "1500 1321 1459 271 359 338 349 682 436 187 996 \n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "The structure of the urls looks like:- (urls-https://www.industrybuying.com/adhesives-sealants-and-tape-2595/)\n",
      "649 7030 361 2857 1308 889 83 172 188 231 342 135 51 60 195 \n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "23\n",
      "25\n",
      "27\n",
      "29\n",
      "The structure of the urls looks like:- (urls-https://www.industrybuying.com/agriculture-garden-landscaping-2384/)\n",
      "1289 834 1123 48 253 48 339 578 223 110 1283 369 71 94 80 252 67 853 3 \n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "23\n",
      "25\n",
      "27\n",
      "29\n",
      "31\n",
      "33\n",
      "35\n",
      "37\n",
      "The structure of the urls looks like:- (urls-https://www.industrybuying.com/appliances-17567/)\n",
      "2642 150 1309 1135 462 142 \n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, i_th_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(urls):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#implement this function and get all the url\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     ith_category_name\u001b[38;5;241m=\u001b[39mcategory_names[index]\n\u001b[0;32m---> 14\u001b[0m     list_subcategories, subcategory_names, logs, error\u001b[38;5;241m=\u001b[39m\u001b[43mfind_all_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_th_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe structure of the urls looks like:- (urls-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_th_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lis \u001b[38;5;129;01min\u001b[39;00m list_subcategories:\n",
      "Cell \u001b[0;32mIn[12], line 118\u001b[0m, in \u001b[0;36mfind_all_urls\u001b[0;34m(given_url)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tries \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxtries):\n\u001b[1;32m    117\u001b[0m     session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m--> 118\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_page_wise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m#prevent from not getting a responce and getting a bad responce\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/Env/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url_having_all_categories=\"https://www.industrybuying.com/categories/\"\n",
    "\n",
    "urls,category_names, error=get_the_category_urls(url_having_all_categories)\n",
    "\n",
    "if error==1:\n",
    "    print(\"There is problem in the url having the categories\")\n",
    "    sys.exit()\n",
    "\n",
    "# urls=['https://www.industrybuying.com/agriculture-garden-landscaping-2384/', 'unknown', 'https://www.industrybuying.com/pumps-1160/', 'https://www.industrybuying.com/solar-4050/', 'please go away', 'https://www.industrybuying.com/welding-552/']\n",
    "    \n",
    "for index, i_th_url in enumerate(urls):\n",
    "    #implement this function and get all the url\n",
    "    ith_category_name=category_names[index]\n",
    "    list_subcategories, subcategory_names, logs, error=find_all_urls(i_th_url)\n",
    "    \n",
    "    print(f\"The structure of the urls looks like:- (urls-{i_th_url})\")\n",
    "    for lis in list_subcategories:\n",
    "        print(len(lis), end=\" \")\n",
    "    print()\n",
    "    \n",
    "    #we have got the logs and error\n",
    "    #if the error ==1, we can simply return the dataframe having the heading as failed \n",
    "    if error==1:\n",
    "        data_logs = {'Category URL':[i_th_url], 'Sub Category URL':[None], 'Category':[ith_category_name], 'Sub Category':[None], \n",
    "                     'Category Check':[1], 'Sub Category Check': [0], 'Pages Check':[0], 'Page Not Retrieved':[\"\"],'Total Product':[0],'Non duplicated Products':[0]}\n",
    "        df = pd.DataFrame(data_logs)\n",
    "        filename=\"IndustryBuying\"+\"__\"+ith_category_name+\"__\"+\"logs.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        #since the data_logs has no value in it, we can dont have the need to define the mandatory, optional and inconsistent urls\n",
    "        continue\n",
    "    #we must define the logs\n",
    "    #since we got the logs, we can simply find the dataframe\n",
    "    logs_table(logs, ith_category_name, subcategory_names, i_th_url)\n",
    "    \n",
    "    \n",
    "    product_with_inconsistency=data_collect(list_subcategories, ith_category_name)\n",
    "    df = pd.DataFrame(product_with_inconsistency, columns=['URL', 'ISSUE'])\n",
    "    df['Category']=ith_category_name\n",
    "    df=df[['Category', 'URL', 'ISSUE']]\n",
    "    filename=\"IndustryBuying\"+\"__\"+ith_category_name+\"__\"+\"InconsistentProducts.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "        \n",
    "    #now all we need to do is get the links which have inconsistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0050eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "\n",
    "directory = os.getcwd()  \n",
    "\n",
    "#lets build dataset for the inconsistent products\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith(\"InconsistentProducts.csv\")]\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    combined_df = pd.concat([df, combined_df])\n",
    "combined_df_reset = combined_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'overall_logs')\n",
    "file_path = os.path.join(directory, 'summarised_InconsistentProducts.csv')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "combined_df_reset.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "#lets do the same for logs\n",
    "# Define the directory where the CSV files are located\n",
    "directory = os.getcwd()  \n",
    "\n",
    "# Get a list of CSV files ending with \"InconsistentProducts\" in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith(\"logs.csv\")]\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "# Loop through each CSV file and append its data to the combined DataFrame\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    combined_df = pd.concat([df, combined_df])\n",
    "combined_df_reset = combined_df.reset_index(drop=True)\n",
    "combined_df_reset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'overall_logs')\n",
    "file_path = os.path.join(directory, 'summarised_logs.csv')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "combined_df_reset.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1a941",
   "metadata": {},
   "source": [
    "# The cell below consist of testing code.\n",
    "# These cells wont be utitlized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e97a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # url_having_all_categories=\"https://www.industrybuying.com/categories/\"\n",
    "\n",
    "# # urls,category_names, error=get_the_category_urls(url_having_all_categories)\n",
    "\n",
    "# # if error==1:\n",
    "# #     print(\"There is problem in the url having the categories\")\n",
    "# #     sys.exit()\n",
    "\n",
    "# urls=['nahi hai']\n",
    "# category_names=['umhmmm']\n",
    "# for index, i_th_url in enumerate(urls):\n",
    "#     #implement this function and get all the url\n",
    "#     ith_category_name=category_names[index]\n",
    "#     list_subcategories, subcategory_names, logs, error=find_all_urls(i_th_url)\n",
    "    \n",
    "#     print(f\"The structure of the urls looks like:- (urls-{i_th_url})\")\n",
    "#     for lis in list_subcategories:\n",
    "#         print(len(lis), end=\" \")\n",
    "#     print()\n",
    "    \n",
    "#     #we have got the logs and error\n",
    "#     #if the error ==1, we can simply return the dataframe having the heading as failed \n",
    "#     if error==1:\n",
    "#         data_logs = {'Category URL':[i_th_url], 'Sub Category URL':[None], 'Category':[ith_category_name], 'Sub Category':[None], \n",
    "#                      'Category Check':[1], 'Sub Category Check': [0], 'Pages Check':[0], 'Page Not Retrieved':[\"\"],'Total Product':[0],'Non duplicated Products':[0]}\n",
    "#         df = pd.DataFrame(data_logs)\n",
    "#         filename=\"IndustryBuying\"+\"__\"+ith_category_name+\"__\"+\"logs.csv\"\n",
    "#         df.to_csv(filename, index=False)\n",
    "#         #since the data_logs has no value in it, we can dont have the need to define the mandatory, optional and inconsistent urls\n",
    "#         continue\n",
    "#     #we must define the logs\n",
    "#     #since we got the logs, we can simply find the dataframe\n",
    "#     logs_table(logs, ith_category_name, subcategory_names, i_th_url)\n",
    "    \n",
    "    \n",
    "#     product_with_inconsistency=data_collect(list_subcategories, ith_category_name)\n",
    "#     df = pd.DataFrame(product_with_inconsistency, columns=['URL', 'ISSUE'])\n",
    "#     df['Category']=ith_category_name\n",
    "#     df=df[['Category', 'URL', 'ISSUE']]\n",
    "#     filename=\"IndustryBuying\"+\"__\"+ith_category_name+\"__\"+\"InconsistentProducts.csv\"\n",
    "#     df.to_csv(filename, index=False)\n",
    "        \n",
    "#     #now all we need to do is get the links which have inconsistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from random import randint\n",
    "\n",
    "\n",
    "# directory = os.getcwd()  \n",
    "\n",
    "# #lets build dataset for the inconsistent products\n",
    "# csv_files = [file for file in os.listdir(directory) if file.endswith(\"InconsistentProducts.csv\")]\n",
    "\n",
    "\n",
    "# combined_df = pd.DataFrame()\n",
    "\n",
    "# for csv_file in csv_files:\n",
    "#     file_path = os.path.join(directory, csv_file)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     combined_df = pd.concat([df, combined_df])\n",
    "# combined_df_reset = combined_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# directory = os.path.join(os.getcwd(), 'overall_logs')\n",
    "# file_path = os.path.join(directory, 'summarised_InconsistentProducts.csv')\n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# combined_df_reset.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "# #lets do the same for logs\n",
    "# # Define the directory where the CSV files are located\n",
    "# directory = os.getcwd()  \n",
    "\n",
    "# # Get a list of CSV files ending with \"InconsistentProducts\" in the directory\n",
    "# csv_files = [file for file in os.listdir(directory) if file.endswith(\"logs.csv\")]\n",
    "\n",
    "\n",
    "# combined_df = pd.DataFrame()\n",
    "# # Loop through each CSV file and append its data to the combined DataFrame\n",
    "# for csv_file in csv_files:\n",
    "#     file_path = os.path.join(directory, csv_file)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     combined_df = pd.concat([df, combined_df])\n",
    "# combined_df_reset = combined_df.reset_index(drop=True)\n",
    "# combined_df_reset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# directory = os.path.join(os.getcwd(), 'overall_logs')\n",
    "# file_path = os.path.join(directory, 'summarised_logs.csv')\n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# combined_df_reset.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ce8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define the directory where the CSV files are located\n",
    "# directory = os.getcwd()  # You can change this to your desired directory\n",
    "\n",
    "# # Get a list of CSV files ending with \"InconsistentProducts\" in the directory\n",
    "# csv_files = [file for file in os.listdir(directory) if file.endswith(\"InconsistentProducts.csv\")]\n",
    "\n",
    "\n",
    "# combined_df = pd.DataFrame()\n",
    "# # Loop through each CSV file and append its data to the combined DataFrame\n",
    "# for csv_file in csv_files:\n",
    "#     file_path = os.path.join(directory, csv_file)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     combined_df = pd.concat([df, combined_df])\n",
    "# combined_df_reset = combined_df.reset_index(drop=True)\n",
    "# combined_df_reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeec5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define the directory where the CSV files are located\n",
    "# directory = os.getcwd()  # You can change this to your desired directory\n",
    "\n",
    "# # Get a list of CSV files ending with \"InconsistentProducts\" in the directory\n",
    "# csv_files = [file for file in os.listdir(directory) if file.endswith(\"logs.csv\")]\n",
    "\n",
    "\n",
    "# combined_df = pd.DataFrame()\n",
    "# # Loop through each CSV file and append its data to the combined DataFrame\n",
    "# for csv_file in csv_files:\n",
    "#     file_path = os.path.join(directory, csv_file)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     combined_df = pd.concat([df, combined_df])\n",
    "# combined_df_reset = combined_df.reset_index(drop=True)\n",
    "# combined_df_reset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
